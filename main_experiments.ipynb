{"cells":[{"cell_type":"markdown","source":["# imports"],"metadata":{"id":"L6GKLJrMztej"},"id":"L6GKLJrMztej"},{"cell_type":"code","execution_count":null,"id":"0175b78a","metadata":{"id":"0175b78a"},"outputs":[],"source":["from abc import ABC, abstractmethod\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, Subset\n","\n","import numpy as np\n","import matplotlib\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","from copy import deepcopy"]},{"cell_type":"markdown","id":"17884805","metadata":{"id":"17884805"},"source":["# utils"]},{"cell_type":"code","execution_count":null,"id":"ffdcdca4","metadata":{"id":"ffdcdca4"},"outputs":[],"source":["def class_accuracy(pred: torch.Tensor, true: torch.Tensor) -> float:\n","    \"\"\"\n","    Computes the percentage class accuracy of the predictions, given the correct\n","    class labels.\n","\n","    Args:\n","        pred: the class predictions made by a model\n","        true: the ground truth classes of the sample\n","    Returns:\n","        Classification accuracy of the predictions w.r.t. the ground truth labels\n","    \"\"\"\n","    return 100 * (pred.int() == true.int()).sum().item() / len(true)\n","\n","\n","def kl_divergence(z_posterior_means, z_posterior_log_std, z_prior_mean=0.0, z_prior_log_std=0.0):\n","    z_prior_means = torch.full_like(z_posterior_means, z_prior_mean)\n","    z_prior_log_stds = torch.full_like(z_posterior_log_std, z_prior_log_std)\n","\n","    prior_precision = torch.exp(torch.mul(z_prior_log_stds, -2))\n","    kl = 0.5 * ((z_posterior_means - z_prior_means) ** 2) * prior_precision - 0.5\n","    kl += z_prior_log_stds - z_posterior_log_std\n","    kl += 0.5 * torch.exp(2 * z_posterior_log_std - 2 * z_prior_log_stds)\n","    return torch.sum(kl, dim=(1,))\n","\n","\n","def bernoulli_log_likelihood(x_observed, x_reconstructed, epsilon=1e-8) -> torch.Tensor:\n","    \"\"\"\n","    For observed batch of data x, and reconstructed data p (we view p as a\n","    probability of a pixel being on), computes a tensor of dimensions\n","    [batch_size] representing the log likelihood of each data point in the batch.\n","    \"\"\"\n","    prob = torch.mul(torch.log(x_reconstructed + epsilon), x_observed)\n","    inv_prob = torch.mul(torch.log(1 - x_reconstructed + epsilon), 1 - x_observed)\n","    inv_prob[inv_prob != inv_prob] = epsilon\n","\n","    return torch.sum(torch.add(prob, inv_prob), dim=(1,))\n","\n","\n","def normal_with_reparameterization(means: torch.Tensor, log_stds: torch.Tensor, device='cpu') -> torch.Tensor:\n","    return torch.add(means, torch.mul(torch.exp(log_stds), torch.randn_like(means)))\n","\n","\n","def concatenate_flattened(tensor_list) -> torch.Tensor:\n","    \"\"\"\n","    Given list of tensors, flattens each and concatenates their values.\n","    \"\"\"\n","    return torch.cat([torch.reshape(t, (-1,)) for t in tensor_list])\n","\n","\n","def task_subset(data: Dataset, task_ids: torch.Tensor, task: int,) -> torch.Tensor:\n","    idx_list = torch.arange(0, len(task_ids))[task_ids == task]\n","    return Subset(data, idx_list)"]},{"cell_type":"code","execution_count":null,"id":"e6ea573e","metadata":{"id":"e6ea573e"},"outputs":[],"source":["import json\n","import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","\n","OUT_DIR = 'experiments/'\n","MODEL_DIR = 'models/'\n","IMAGE_DIR = 'images/'\n","\n","\n","def write_as_json(filename, data):\n","    \"\"\"\n","    Dumps the given data into the specified file using JSON formatting. The file\n","    is created if it does not exist.\n","\n","    Args:\n","        filename: path to file to dump JSON into\n","        data: numeric data to dump\n","    \"\"\"\n","    if not os.path.exists(os.path.dirname(OUT_DIR + filename)):\n","        print('creating ...')\n","        os.makedirs(os.path.dirname(OUT_DIR + filename))\n","\n","    with open(OUT_DIR + filename, \"w\") as f:\n","        json.dump(data, f)\n","\n","\n","def save_model(model, filename):\n","    if not os.path.exists(os.path.dirname(MODEL_DIR)):\n","        print('creating ...')\n","        os.makedirs(os.path.dirname(MODEL_DIR))\n","\n","    torch.save(model, MODEL_DIR + filename)\n","\n","\n","def load_model(filename):\n","    if not os.path.exists(os.path.dirname(MODEL_DIR)):\n","        raise FileNotFoundError()\n","    return torch.load(MODEL_DIR + filename)\n","\n","\n","def save_generated_image(data: np.ndarray, filename: str):\n","    if not os.path.exists(os.path.dirname(IMAGE_DIR)):\n","        print('creating ...')\n","        os.makedirs(os.path.dirname(IMAGE_DIR))\n","\n","    data = data * 255\n","    image = Image.fromarray(data)\n","    if image.mode != 'RGB':\n","        image = image.convert('RGB')\n","    image.save(IMAGE_DIR + filename)\n","    np.save(IMAGE_DIR + filename + str('.npy'), data)"]},{"cell_type":"code","execution_count":null,"id":"8a01589b","metadata":{"id":"8a01589b"},"outputs":[],"source":["import numpy as np\n","\n","\n","class Flatten(object):\n","    \"\"\" Transforms a PIL image to a flat numpy array. \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, sample):\n","        return np.array(sample, dtype=np.float32).flatten()\n","\n","\n","class Scale(object):\n","    \"\"\"Scale images down to have [0,1] float pixel values\"\"\"\n","    def __init__(self, max_value=255):\n","        self.max_value = max_value\n","\n","    def __call__(self, sample):\n","        return sample / self.max_value\n","\n","\n","class Permute(object):\n","    \"\"\" Apply a fixed permutation to the pixels in the image. \"\"\"\n","    def __init__(self, permutation):\n","        self.permutation = permutation\n","\n","    def __call__(self, sample):\n","        return sample[self.permutation]"]},{"cell_type":"markdown","id":"2002a770","metadata":{"id":"2002a770"},"source":["# run_task"]},{"cell_type":"code","execution_count":null,"id":"c8b9c678","metadata":{"id":"c8b9c678"},"outputs":[],"source":["\"\"\"\n","Utilities that abstract the low-level details of experiments, such as standard train-and-eval loops.\n","\"\"\"\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Subset\n","from tqdm import tqdm\n","\n","\n","def run_point_estimate_initialisation(model, data, epochs, task_ids, batch_size,\n","                                      device, lr, task_idx=0, y_transform=None,\n","                                      multiheaded=True):\n","    print(\"Obtaining point estimate for posterior initialisation\")\n","\n","    head = task_idx if multiheaded else 0\n","\n","    # each task has its own optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    # obtain the appropriate data subset depending on which task we are running\n","    task_data = task_subset(data, task_ids, task_idx)\n","    loader = DataLoader(task_data, batch_size)\n","\n","    # train\n","    for _ in tqdm(range(epochs), 'Epochs: '):\n","        for batch in loader:\n","            optimizer.zero_grad()\n","            x, y_true = batch\n","            x = x.to(device)\n","            y_true = y_true.to(device)\n","\n","            if y_transform is not None:\n","                y_true = y_transform(y_true, task_idx).to(device)\n","\n","            loss = model.point_estimate_loss(x, y_true, head=head)\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","def run_task(model, train_data, train_task_ids, test_data, test_task_ids,\n","             task_idx, coreset, epochs, batch_size, save_as, device, lr,\n","             y_transform=None, multiheaded=True, train_full_coreset=True,\n","             summary_writer=None):\n","\n","\n","    print('TASK ', task_idx)\n","\n","    # separate optimizer for each task\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    head = task_idx if multiheaded else 0\n","\n","    # obtain correct subset of data for training, and set some aside for the coreset\n","    task_data = task_subset(train_data, train_task_ids, task_idx)\n","    non_coreset_data = coreset.select(task_data, task_id=task_idx)\n","    train_loader = DataLoader(non_coreset_data, batch_size)\n","\n","    # train\n","    for epoch in tqdm(range(epochs), 'Epochs: '):\n","        epoch_loss = 0\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            x, y_true = batch\n","            x = x.to(device)\n","            y_true = y_true.to(device)\n","\n","            if y_transform is not None:\n","                y_true = y_transform(y_true, task_idx)\n","\n","            loss = model.vcl_loss(x, y_true, head, len(task_data))\n","            epoch_loss += len(x) * loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        if summary_writer is not None:\n","            summary_writer.add_scalars(\"loss\", {\"TASK_\" + str(task_idx): epoch_loss / len(task_data)}, epoch)\n","\n","    # after training, prepare for new task by copying posteriors into priors\n","    model.reset_for_new_task(head)\n","\n","    # train using full coreset\n","    if train_full_coreset:\n","        model_cs_trained = coreset.coreset_train(\n","            model, optimizer, list(range(task_idx + 1)), epochs,\n","            device, y_transform=y_transform, multiheaded=multiheaded)\n","\n","    # test\n","    task_accuracies = []\n","    tot_right = 0\n","    tot_tested = 0\n","\n","    for test_task_idx in range(task_idx + 1):\n","        if not train_full_coreset:\n","            model_cs_trained = coreset.coreset_train(\n","                model, optimizer, test_task_idx, epochs,\n","                device, y_transform=y_transform, multiheaded=multiheaded)\n","\n","        head = test_task_idx if multiheaded else 0\n","\n","        task_data = task_subset(test_data, test_task_ids, test_task_idx)\n","\n","        x = torch.Tensor([x for x, _ in task_data])\n","        y_true = torch.Tensor([y for _, y in task_data])\n","        x = x.to(device)\n","        y_true = y_true.to(device)\n","\n","        if y_transform is not None:\n","            y_true = y_transform(y_true, test_task_idx)\n","\n","        y_pred = model_cs_trained.prediction(x, head).to(device)\n","\n","        acc = class_accuracy(y_pred, y_true)\n","        print(\"After task {} perfomance on task {} is {}\"\n","              .format(task_idx, test_task_idx, acc))\n","\n","        tot_right += acc * len(task_data)\n","        tot_tested += len(task_data)\n","        task_accuracies.append(acc)\n","\n","    mean_accuracy = tot_right / tot_tested\n","    print(\"Mean accuracy:\", mean_accuracy)\n","\n","    return task_accuracies, mean_accuracy\n","\n","def run_task_scale(model, train_data, train_task_ids, test_data, test_task_ids,\n","             task_idx, coreset, epochs, batch_size, save_as, device, lr,\n","             y_transform=None, multiheaded=True, train_full_coreset=True,\n","             summary_writer=None):\n","\n","\n","    print('TASK ', task_idx)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    head = task_idx if multiheaded else 0\n","\n","    task_data = task_subset(train_data, train_task_ids, task_idx)\n","    non_coreset_data = coreset.select(task_data, task_id=task_idx)\n","    train_loader = DataLoader(non_coreset_data, batch_size)\n","\n","    for epoch in tqdm(range(epochs), 'Epochs: '):\n","        epoch_loss = 0\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            x, y_true = batch\n","            x = x.to(device)\n","            y_true = y_true.to(device)\n","\n","            if y_transform is not None:\n","                y_true = y_transform(y_true, task_idx)\n","\n","            loss = model.vcl_loss_factor(x, y_true, head, len(task_data))\n","            epoch_loss += len(x) * loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        if summary_writer is not None:\n","            summary_writer.add_scalars(\"loss\", {\"TASK_\" + str(task_idx): epoch_loss / len(task_data)}, epoch)\n","\n","    model.reset_for_new_task(head)\n","\n","    if train_full_coreset:\n","        model_cs_trained = coreset.coreset_train(\n","            model, optimizer, list(range(task_idx + 1)), epochs,\n","            device, y_transform=y_transform, multiheaded=multiheaded)\n","\n","    task_accuracies = []\n","    tot_right = 0\n","    tot_tested = 0\n","\n","    for test_task_idx in range(task_idx + 1):\n","        if not train_full_coreset:\n","            model_cs_trained = coreset.coreset_train(\n","                model, optimizer, test_task_idx, epochs,\n","                device, y_transform=y_transform, multiheaded=multiheaded)\n","\n","        head = test_task_idx if multiheaded else 0\n","\n","        task_data = task_subset(test_data, test_task_ids, test_task_idx)\n","\n","        x = torch.Tensor([x for x, _ in task_data])\n","        y_true = torch.Tensor([y for _, y in task_data])\n","        x = x.to(device)\n","        y_true = y_true.to(device)\n","\n","        if y_transform is not None:\n","            y_true = y_transform(y_true, test_task_idx)\n","\n","        y_pred = model_cs_trained.prediction(x, head).to(device)\n","\n","        acc = class_accuracy(y_pred, y_true)\n","        print(\"After task {} perfomance on task {} is {}\"\n","              .format(task_idx, test_task_idx, acc))\n","\n","        tot_right += acc * len(task_data)\n","        tot_tested += len(task_data)\n","        task_accuracies.append(acc)\n","\n","    mean_accuracy = tot_right / tot_tested\n","    print(\"Mean accuracy:\", mean_accuracy)\n","\n","    return task_accuracies, mean_accuracy\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0bcdc32f","metadata":{"id":"0bcdc32f"},"outputs":[],"source":["import torch\n","import torch.utils.data as data\n","from copy import deepcopy\n","import numpy as np\n","import torch.optim as optim\n","from tqdm import tqdm\n","from random import shuffle\n"]},{"cell_type":"markdown","id":"886bf3d1","metadata":{"id":"886bf3d1"},"source":["# DiscriminativeVCL"]},{"cell_type":"code","execution_count":null,"id":"a4c54ee9","metadata":{"id":"a4c54ee9"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","TRAIN_NUM_SAMPLES = 50\n","TEST_NUM_SAMPLES = 50\n","EPSILON = 1e-8\n","\n","class DiscriminativeVCL(nn.Module):\n","    \"\"\"\n","    A Bayesian multi-head neural network which updates its parameters using\n","    variational inference.\n","    \"\"\"\n","\n","    def __init__(self, in_size: int, out_size: int, layer_width: int,\n","                 n_hidden_layers: int, n_heads: int, initial_posterior_var: int):\n","        super().__init__()\n","        self.input_size = in_size\n","        self.out_size = out_size\n","        self.n_hidden_layers = n_hidden_layers\n","        self.layer_width = layer_width\n","        self.n_heads = n_heads\n","\n","        print(\"Number of heads:\", n_heads)\n","\n","        self.prior, self.posterior = None, None\n","        self.head_prior, self.head_posterior = None, None\n","\n","        self._init_variables(initial_posterior_var)\n","        self.initial_alpha = 1.0\n","        self.initial_beta = 1.0\n","        # Initialize current alpha and beta to the initial values\n","        self.alpha = self.initial_alpha\n","        self.beta = self.initial_beta\n","\n","    def adjust_scaling_factors(self, mean_accuracy, new_task_accuracy):\n","        \"\"\"\n","        Adjusts the scaling factors based on the comparison between\n","        previous task accuracy and current task accuracy.\n","        \"\"\"\n","        perform_diff = new_task_accuracy- mean_accuracy\n","        alpha_adjustment_rate = 1.0 - perform_diff/10\n","        beta_adjustment_rate = 1.0 + perform_diff/10\n","\n","        # Ensure the rates are within sensible bounds\n","        alpha_adjustment_rate = max(0.5, min(alpha_adjustment_rate, 1.5))\n","        beta_adjustment_rate = max(0.5, min(beta_adjustment_rate, 1.5))\n","\n","        self.alpha = self.initial_alpha * alpha_adjustment_rate\n","        self.beta = self.initial_beta * beta_adjustment_rate\n","        print('alpha', self.alpha)\n","        print('beta', self.beta)\n","\n","    def adjust_scaling_factors_2(self, new_task_idx, mean_accuracy, new_task_accuracy, task_similarity_matrix):\n","        \"\"\"\n","        Adjusts the scaling factors based on the comparison between\n","        previous task accuracy and current task accuracy.\n","        \"\"\"\n","        perform_diff = new_task_accuracy- mean_accuracy\n","        alpha_adjustment_rate = 1.0 - perform_diff/10\n","        beta_adjustment_rate = 1.0 + perform_diff/10\n","\n","        # Ensure the rates are within sensible bounds\n","        alpha_adjustment_rate = max(0.5, min(alpha_adjustment_rate, 1.5))\n","        beta_adjustment_rate = max(0.5, min(beta_adjustment_rate, 1.5))\n","\n","        self.alpha = self.initial_alpha * alpha_adjustment_rate\n","        self.beta = self.initial_beta * beta_adjustment_rate\n","        print('alpha', self.alpha)\n","        print('beta', self.beta)\n","\n","    def to(self, *args, **kwargs):\n","        \"\"\"\n","        Our prior tensors are registered as buffers but the way we access them\n","        indirectly (through tuple attributes on the model) is causing problems\n","        because when we use `.to()` to move the model to a new device, the prior\n","        tensors get moved (because they're registered as buffers) but the\n","        references in the tuples don't get updated to point to the new moved\n","        tensors. This has no effect when running just on a cpu but breaks the\n","        model when trying to run on a gpu. There are a million nicer ways of\n","        working around this problem, but for now the easiest thing is to do\n","        this: override the `.to()` method and manually update our references to\n","        prior tensors.\n","        \"\"\"\n","        self = super().to(*args, **kwargs)\n","        (prior_w_means, prior_w_log_vars), (prior_b_means, prior_b_log_vars) = self.prior\n","        prior_w_means = [t.to(*args, **kwargs) for t in prior_w_means]\n","        prior_w_log_vars = [t.to(*args, **kwargs) for t in prior_w_log_vars]\n","        prior_b_means = [t.to(*args, **kwargs) for t in prior_b_means]\n","        prior_b_log_vars = [t.to(*args, **kwargs) for t in prior_b_log_vars]\n","        self.prior = (prior_w_means, prior_w_log_vars), (prior_b_means, prior_b_log_vars)\n","        (head_prior_w_means, head_prior_w_log_vars), (head_prior_b_means, head_prior_b_log_vars) = self.head_prior\n","        head_prior_w_means = [t.to(*args, **kwargs) for t in head_prior_w_means]\n","        head_prior_w_log_vars = [t.to(*args, **kwargs) for t in head_prior_w_log_vars]\n","        head_prior_b_means = [t.to(*args, **kwargs) for t in head_prior_b_means]\n","        head_prior_b_log_vars = [t.to(*args, **kwargs) for t in head_prior_b_log_vars]\n","        self.head_prior = (head_prior_w_means, head_prior_w_log_vars), (head_prior_b_means, head_prior_b_log_vars)\n","        return self\n","\n","    def forward(self, x, head):\n","        \"\"\" Forward pass of the model on an input. \"\"\"\n","        # sample layer parameters from posterior distribution\n","        (w_means, w_log_vars), (b_means, b_log_vars) = self.posterior\n","        (head_w_means, head_w_log_vars), (head_b_means, head_b_log_vars) = self.head_posterior\n","        sampled_layers = self._sample_parameters(w_means, b_means, w_log_vars, b_log_vars)\n","        sampled_head_layers = self._sample_parameters(head_w_means, head_b_means, head_w_log_vars, head_b_log_vars)\n","\n","        # Apply each layer with its sampled weights and biases\n","        for weight, bias in sampled_layers:\n","            x = F.relu(x @ weight + bias)\n","\n","        head_weight, head_bias = list(sampled_head_layers)[head]\n","        x = x @ head_weight + head_bias\n","\n","        return x\n","\n","    def vcl_loss(self, x, y, head, task_size, num_samples=TRAIN_NUM_SAMPLES) -> torch.Tensor:\n","\n","        return self._calculate_kl_term(head).cpu() / task_size - self._log_prob(x, y, head, num_samples)\n","\n","    def vcl_loss2(self, x, y, head, task_size, num_samples=TRAIN_NUM_SAMPLES, similarity_score=1.0) -> torch.Tensor:\n","        kl_term = self._calculate_kl_term(head) / task_size\n","        log_prob = self._log_prob(x, y, head, num_samples)\n","        dynamic_kl_weight = 1.0 + (1.0 - similarity_score) * adaptability_factor\n","        return dynamic_kl_weight * kl_term - log_prob\n","\n","    def vcl_loss_factor(self, x, y, head, task_size, num_samples=TRAIN_NUM_SAMPLES):\n","        kl_term = self._calculate_kl_term(head) / task_size\n","        log_prob = self._log_prob(x, y, head, num_samples)\n","        return self.beta * kl_term - self.alpha * log_prob\n","\n","    def point_estimate_loss(self, x, y, head=0):\n","        \"\"\"\n","        Returns a loss defined in terms of a simplified forward pass that\n","        doesn't use sampling, and so uses the posterior means but not the\n","        variances. Used as part of model initialisation to optimise the\n","        posterior means to point-estimates for the first head.\n","        \"\"\"\n","        (w_means, _), (b_means, _) = self.posterior\n","        (head_w_means, _), (head_b_means, _) = self.head_posterior\n","\n","        for weight, bias in zip(w_means, b_means):\n","            x = F.relu(x @ weight + bias)\n","\n","        x = x @ head_w_means[head] + head_b_means[head]\n","\n","        return nn.CrossEntropyLoss()(x, y)\n","\n","    def prediction(self, x, head, num_samples=TEST_NUM_SAMPLES):\n","        \"\"\"Returns an integer between 0 and self.out_size\"\"\"\n","        outputs = torch.empty(num_samples, len(x), self.out_size)\n","        for i in range(num_samples):\n","            outputs[i] = nn.Softmax(dim=1)(self.forward(x, head))\n","\n","        predictions = outputs.mean(dim=0)\n","\n","        return torch.argmax(predictions, dim=1)\n","\n","    def reset_for_new_task(self, head):\n","        \"\"\"\n","        Called after completion of a task, to reset state for the next task\n","        \"\"\"\n","        # Set the value of the prior to be the current value of the posterior\n","        (prior_w_means, prior_w_log_vars), (prior_b_means, prior_b_log_vars) = self.prior\n","        (post_w_means, post_w_log_vars), (post_b_means, post_b_log_vars) = self.posterior\n","        for i in range(self.n_hidden_layers):\n","            prior_w_means[i].data.copy_(post_w_means[i].data)\n","            prior_w_log_vars[i].data.copy_(post_w_log_vars[i].data)\n","            prior_b_means[i].data.copy_(post_b_means[i].data)\n","            prior_b_log_vars[i].data.copy_(post_b_log_vars[i].data)\n","\n","        # set the value of the head prior to be the current value of the posterior\n","        (head_prior_w_means, head_prior_w_log_vars), (head_prior_b_means, head_prior_b_log_vars) = self.head_prior\n","        (head_posterior_w_means, head_posterior_w_log_vars), (head_posterior_b_means, head_posterior_b_log_vars) = self.head_posterior\n","        head_prior_w_means[head].data.copy_(head_posterior_w_means[head].data)\n","        head_prior_w_log_vars[head].data.copy_(head_posterior_w_log_vars[head].data)\n","        head_prior_b_means[head].data.copy_(head_posterior_b_means[head].data)\n","        head_prior_b_log_vars[head].data.copy_(head_posterior_b_log_vars[head].data)\n","\n","    def _calculate_kl_term(self, head):\n","        \"\"\"\n","        Calculates and returns the KL divergence of the new posterior and the previous\n","        iteration's posterior. See equation L3, slide 14.\n","        \"\"\"\n","        # Prior\n","        ((prior_w_means, prior_w_log_vars), (prior_b_means, prior_b_log_vars)) = self.prior\n","        ((head_prior_w_means, head_prior_w_log_vars),\n","         (head_prior_b_means, head_prior_b_log_vars)) = self.head_prior\n","\n","        prior_means = concatenate_flattened(\n","            prior_w_means + head_prior_w_means[head:head+1] +\n","            prior_b_means + head_prior_b_means[head:head+1])\n","        prior_log_vars = concatenate_flattened(\n","            prior_w_log_vars + head_prior_w_log_vars[head:head+1] +\n","            prior_b_log_vars + head_prior_b_log_vars[head:head+1])\n","        prior_vars = torch.exp(prior_log_vars)\n","\n","        # Posterior\n","        ((post_w_means, post_w_log_vars), (post_b_means, post_b_log_vars)) = self.posterior\n","        ((head_post_w_means, head_post_w_log_vars),\n","         (head_post_b_means, head_post_b_log_vars)) = self.head_posterior\n","\n","        post_means = concatenate_flattened(\n","            post_w_means + head_post_w_means[head:head+1] +\n","            post_b_means + head_post_b_means[head:head+1])\n","        post_log_vars = concatenate_flattened(\n","            post_w_log_vars + head_post_w_log_vars[head:head+1] +\n","            post_b_log_vars + head_post_b_log_vars[head:head+1])\n","        post_vars = torch.exp(post_log_vars)\n","\n","        # Calculate KL for individual normal distributions over parameters\n","        kl_elementwise = \\\n","            post_vars / (prior_vars + EPSILON) + \\\n","            torch.pow(prior_means - post_means, 2) / (prior_vars + EPSILON) \\\n","            - 1 + prior_log_vars - post_log_vars\n","\n","        # Sum KL over all parameters\n","        return 0.5 * kl_elementwise.sum()\n","\n","    def _log_prob(self, x, y, head, num_samples):\n","        outputs = []\n","        for i in range(num_samples):\n","            outputs.append(self.forward(x, head))\n","\n","        return - nn.CrossEntropyLoss()(torch.cat(outputs), y.repeat(num_samples).view(-1))\n","\n","    def _sample_parameters(self, w_means, b_means, w_log_vars, b_log_vars):\n","        # sample weights and biases from normal distributions\n","        sampled_weights, sampled_bias = [], []\n","        for layer_n in range(len(w_means)):\n","            w_epsilons = torch.randn_like(w_means[layer_n])\n","            b_epsilons = torch.randn_like(b_means[layer_n])\n","            sampled_weights.append(w_means[layer_n] + w_epsilons * torch.exp(0.5 * w_log_vars[layer_n]))\n","            sampled_bias.append(b_means[layer_n] + b_epsilons * torch.exp(0.5 * b_log_vars[layer_n]))\n","        return zip(sampled_weights, sampled_bias)\n","\n","    def _init_variables(self, initial_posterior_var):\n","        \"\"\"\n","        Initializes the model's prior and posterior weights / biases to their initial\n","        values. This method is called once on model creation. The model prior is registered\n","        as a persistent part of the model state which should not be modified, while the\n","        initial posterior is registered as a model parameter to be optimized.\n","\n","        To avoid negative variances, we do not store parameter variances directly; instead\n","        we store the logarithm of each variance, and apply the exponential as needed in the\n","        forward pass.\n","        \"\"\"\n","        # The initial prior over the parameters has zero mean, unit variance (i.e. log variance 0)\n","        prior_w_means = [torch.zeros(self.input_size, self.layer_width)] + \\\n","                        [torch.zeros(self.layer_width, self.layer_width) for _ in range(self.n_hidden_layers - 1)]\n","        prior_w_log_vars = [torch.zeros_like(t) for t in prior_w_means]\n","        prior_b_means = [torch.zeros(self.layer_width) for _ in range(self.n_hidden_layers)]\n","        prior_b_log_vars = [torch.zeros_like(t) for t in prior_b_means]\n","\n","        self.prior = ((prior_w_means, prior_w_log_vars), (prior_b_means, prior_b_log_vars))\n","\n","        head_prior_w_means = [torch.zeros(self.layer_width, self.out_size) for t in range(self.n_heads)]\n","        head_prior_w_log_vars = [torch.zeros_like(t) for t in head_prior_w_means]\n","        head_prior_b_means = [torch.zeros(self.out_size) for t in range(self.n_heads)]\n","        head_prior_b_log_vars = [torch.zeros_like(t) for t in head_prior_b_means]\n","\n","        self.head_prior = ((head_prior_w_means, head_prior_w_log_vars), (head_prior_b_means, head_prior_b_log_vars))\n","\n","        empty_parameter_like = lambda t: nn.Parameter(torch.empty_like(t, requires_grad=True))\n","\n","        posterior_w_means = [empty_parameter_like(t) for t in prior_w_means]\n","        posterior_w_log_vars = [empty_parameter_like(t) for t in prior_w_log_vars]\n","        posterior_b_means = [empty_parameter_like(t) for t in prior_b_means]\n","        posterior_b_log_vars = [empty_parameter_like(t) for t in prior_b_log_vars]\n","\n","        self.posterior = ((posterior_w_means, posterior_w_log_vars), (posterior_b_means, posterior_b_log_vars))\n","\n","        head_posterior_w_means = [empty_parameter_like(t) for t in head_prior_w_means]\n","        head_posterior_w_log_vars = [empty_parameter_like(t) for t in head_prior_w_log_vars]\n","        head_posterior_b_means = [empty_parameter_like(t) for t in head_prior_b_means]\n","        head_posterior_b_log_vars = [empty_parameter_like(t) for t in head_prior_b_log_vars]\n","\n","        self.head_posterior = \\\n","            ((head_posterior_w_means, head_posterior_w_log_vars),\n","             (head_posterior_b_means, head_posterior_b_log_vars))\n","\n","        for t in posterior_w_means + posterior_b_means + head_posterior_w_means + head_posterior_b_means:\n","            torch.nn.init.normal_(t, mean=0, std=0.1)\n","\n","        for t in posterior_w_log_vars + posterior_b_log_vars + head_posterior_w_log_vars + head_posterior_b_log_vars:\n","            torch.nn.init.constant_(t, math.log(initial_posterior_var))\n","\n","        for i in range(self.n_hidden_layers):\n","            self.register_buffer(\"prior_w_means_\" + str(i), prior_w_means[i])\n","            self.register_buffer(\"prior_w_log_vars_\" + str(i), prior_w_log_vars[i])\n","            self.register_buffer(\"prior_b_means_\" + str(i), prior_b_means[i])\n","            self.register_buffer(\"prior_b_log_vars_\" + str(i), prior_b_log_vars[i])\n","\n","        for i in range(self.n_heads):\n","            self.register_buffer(\"head_prior_w_means_\" + str(i), head_prior_w_means[i])\n","            self.register_buffer(\"head_prior_w_log_vars_\" + str(i), head_prior_w_log_vars[i])\n","            self.register_buffer(\"head_prior_b_means_\" + str(i), head_prior_b_means[i])\n","            self.register_buffer(\"head_prior_b_log_vars_\" + str(i), head_prior_b_log_vars[i])\n","\n","        for i in range(self.n_hidden_layers):\n","            self.register_parameter(\"posterior_w_means_\" + str(i), posterior_w_means[i])\n","            self.register_parameter(\"posterior_w_log_vars_\" + str(i), posterior_w_log_vars[i])\n","            self.register_parameter(\"posterior_b_means_\" + str(i), posterior_b_means[i])\n","            self.register_parameter(\"posterior_b_log_vars_\" + str(i), posterior_b_log_vars[i])\n","\n","        for i in range(self.n_heads):\n","            self.register_parameter(\"head_posterior_w_means_\" + str(i), head_posterior_w_means[i])\n","            self.register_parameter(\"head_posterior_w_log_vars_\" + str(i), head_posterior_w_log_vars[i])\n","            self.register_parameter(\"head_posterior_b_means_\" + str(i), head_posterior_b_means[i])\n","            self.register_parameter(\"head_posterior_b_log_vars_\" + str(i), head_posterior_b_log_vars[i])\n","\n","    def _mean_posterior_variance(self):\n","        \"\"\"\n","        Return the mean posterior variance for logging purposes.\n","        Excludes the head layer.\n","        \"\"\"\n","        ((_, posterior_w_log_vars), (_, posterior_b_log_vars)) = self.posterior\n","        posterior_log_vars = torch.cat([torch.reshape(t, (-1,)) for t in posterior_w_log_vars] + posterior_b_log_vars)\n","        posterior_vars     = torch.exp(posterior_log_vars)\n","        return torch.mean(posterior_vars).item()\n","\n","\n"]},{"cell_type":"markdown","id":"f0c06b7e","metadata":{"id":"f0c06b7e"},"source":["# coreset"]},{"cell_type":"code","execution_count":null,"id":"20a3356c","metadata":{"id":"20a3356c"},"outputs":[],"source":["import torch\n","import torch.utils.data as data\n","from copy import deepcopy\n","import numpy as np\n","import torch.optim as optim\n","from tqdm import tqdm\n","from random import shuffle\n","import torch\n","from torch.utils.data import Dataset, Subset, ConcatDataset, TensorDataset\n","import random\n","\n","class Coreset():\n","    \"\"\"\n","    Base class for the the coreset.  This version of the class has no\n","    coreset but subclasses will replace the select method.\n","    \"\"\"\n","\n","    def __init__(self, size=0, lr=0.001):\n","        self.size = size\n","        self.coreset = None\n","        self.coreset_task_ids = None\n","        self.lr = lr\n","\n","    def select(self, d: data.Dataset, task_id: int):\n","        \"\"\"\n","        Given a torch dataset, will choose k datapoints.  Will then update\n","        the coreset with these datapoints.\n","        Returns: the subset that was not selected as a torch dataset.\n","        \"\"\"\n","\n","        return d\n","\n","    def coreset_train(self, m, old_optimizer, tasks, epochs, device,\n","                      y_transform=None, multiheaded=True, batch_size=256):\n","        \"\"\"\n","        Returns a new model, trained on the coreset.  The returned model will\n","        be a deep copy, except when coreset is empty (when it will be identical)\n","\n","        tasks can be either a list, in which case the coreset will be trained\n","        on all tasks in the list, or an integer, in which case it will be\n","        trained on only that task.\n","        \"\"\"\n","\n","        if self.coreset is None:\n","            return m\n","\n","        model = deepcopy(m)\n","\n","        optimizer = optim.Adam(model.parameters(), lr=self.lr)\n","        optimizer.load_state_dict(old_optimizer.state_dict())\n","\n","        # if tasks is an integer, turn it into a singleton.\n","        if isinstance(tasks, int):\n","            tasks = [tasks]\n","\n","        # create dict of train_loaders\n","        train_loaders = {\n","            task_idx :  data.DataLoader(\n","                            task_subset(self.coreset, self.coreset_task_ids, task_idx),\n","                            batch_size\n","                        )\n","            for task_idx in tasks\n","        }\n","\n","        print('CORESET TRAIN')\n","        for _ in tqdm(range(epochs), 'Epochs: '):\n","            # Randomize order of training tasks\n","            shuffle(tasks)\n","            for task_idx in tasks:\n","                head = task_idx if multiheaded else 0\n","\n","                for batch in train_loaders[task_idx]:\n","                    optimizer.zero_grad()\n","                    x, y_true = batch\n","                    x = x.to(device)\n","                    y_true = y_true.to(device)\n","\n","                    if y_transform is not None:\n","                        y_true = y_transform(y_true, task_idx)\n","\n","                    loss = model.vcl_loss(x, y_true, head, len(self.coreset))\n","                    loss.backward()\n","                    optimizer.step()\n","\n","        return model\n","\n","\n","\n","class RandomCoreset(Coreset):\n","\n","    def __init__(self, size):\n","        super().__init__(size)\n","\n","    def select(self, d : data.Dataset, task_id : int):\n","#         print([_ for x, _ in d])\n","\n","        new_cs_data, non_cs = data.random_split(d, [self.size, max(0,len(d)-self.size)])\n","\n","        # Need to split the x from the y values to also include the task values.\n","        # I don't like this way of doing it, but I couldn't find something better.\n","        new_cs_x = torch.tensor([x for x, _ in new_cs_data])\n","        new_cs_y = torch.tensor([y for _, y in new_cs_data])\n","\n","        new_cs = data.TensorDataset(new_cs_x, new_cs_y)\n","        new_task_ids = torch.full((len(new_cs_data),), task_id)\n","\n","        if self.coreset is None:\n","            self.coreset = new_cs\n","            self.coreset_task_ids = new_task_ids\n","        else:\n","            self.coreset = data.ConcatDataset((self.coreset, new_cs))\n","            self.coreset_task_ids = torch.cat((self.coreset_task_ids, new_task_ids))\n","\n","        return non_cs\n","\n","# ---------------------------------------------------------------------------------------------------------\n","\n","import torch\n","from torch.utils.data import Dataset, ConcatDataset, DataLoader, random_split\n","\n","\n","class RandomCoreset_resample(Coreset):\n","    def __init__(self, size, dataset, lr=0.001):\n","        super().__init__(size)\n","        self.size = size\n","        self.coreset_task_ids = torch.tensor([], dtype=torch.long)  # Store (dataset, task_id) for all tasks\n","        self.all_data_x = [x for x, _ in dataset]\n","        self.all_data_y = [y for _, y in dataset]\n","        self.dataset = dataset\n","        self.lr = lr\n","\n","\n","    def select(self, d: Dataset, task_id):\n","        label_to_task_mapping = {\n","        0: 0, 1: 0,\n","        2: 1, 3: 1,\n","        4: 2, 5: 2,\n","        6: 3, 7: 3,\n","        8: 4, 9: 4,\n","    }\n","        task_to_labels = {task: [] for task in set(label_to_task_mapping.values())}\n","        for label, task in label_to_task_mapping.items():\n","            task_to_labels[task].append(label)\n","\n","        current_classes = [label for label, task in label_to_task_mapping.items() if task <= task_id]\n","\n","        print('Current classes are:', current_classes)\n","\n","        num_classes = 10\n","        samples_per_class = self.size // num_classes\n","\n","        new_coreset_data = []\n","\n","        selected_indices = []\n","        all_current_class_indices = []\n","        for class_id in current_classes:\n","            # Find indices of this class\n","            class_indices = [i for i, y in enumerate(self.all_data_y) if y == class_id]\n","            selected_for_class = random.sample(class_indices, min(samples_per_class, len(class_indices)))\n","            all_current_class_indices.extend(class_indices)\n","            selected_indices.extend(selected_for_class) # the selected indices for current class\n","        print(len(selected_indices))\n","        # Extract selected samples for coreset\n","        self.all_data_x = [torch.tensor(x) if not isinstance(x, torch.Tensor) else x for x in self.all_data_x]\n","        self.all_data_y = [torch.tensor(y, dtype=torch.long) if not isinstance(y, torch.Tensor) else y for y in self.all_data_y]\n","\n","        self.new_x = [torch.tensor(x) if not isinstance(x, torch.Tensor) else x for x in self.all_data_x]\n","        self.new_y = [torch.tensor(y, dtype=torch.long) if not isinstance(y, torch.Tensor) else y for y in self.all_data_y]\n","\n","        coreset_x = torch.stack([self.new_x[i] for i in selected_indices])\n","        coreset_y = torch.tensor([self.new_y[i] for i in selected_indices], dtype=torch.long)\n","\n","        # Update coreset and task ids\n","        new_coreset = TensorDataset(coreset_x, coreset_y)\n","        new_task_ids = torch.full((len(new_coreset),), task_id, dtype=torch.long)\n","\n","        # if self.coreset is None:\n","        self.coreset = new_coreset\n","        self.coreset_task_ids = new_task_ids\n","        # else:\n","        #     self.coreset = ConcatDataset([self.coreset, new_coreset])\n","        #     self.coreset_task_ids = torch.cat((self.coreset_task_ids, new_task_ids))\n","        print(\"len(self.coreset)\", len(self.coreset))\n","\n","#         ye_corset = self.coreset\n","\n","#         non_selected_indices = list(set(range(len(self.all_data_y))) - set(selected_indices))\n","        non_selected_indices = set(all_current_class_indices) - set(selected_indices)\n","#         non_coreset_x = [self.all_data_x[i] for i in non_selected_indices]\n","#         non_coreset_y = [self.all_data_y[i] for i in non_selected_indices]\n","        non_coreset_x = torch.stack([self.all_data_x[i] for i in non_selected_indices])\n","        non_coreset_y = torch.tensor([self.all_data_y[i] for i in non_selected_indices], dtype=torch.long)\n","#         non_coreset = TensorDataset(torch.stack(non_coreset_x), torch.tensor(non_coreset_y))\n","        non_coreset = TensorDataset(non_coreset_x, non_coreset_y)\n","        print('non_coreset', len(non_coreset))\n","\n","\n","        return non_coreset"]},{"cell_type":"markdown","id":"8017dfda","metadata":{"id":"8017dfda"},"source":["# experiment"]},{"cell_type":"code","execution_count":null,"id":"6728128a","metadata":{"id":"6728128a"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import numpy as np\n","from torchvision.datasets import MNIST, CIFAR100\n","from torchvision.transforms import Compose\n","from torch.utils.data import ConcatDataset\n","from tensorboardX import SummaryWriter\n","import os\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"id":"e99460a4","metadata":{"id":"e99460a4"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"9c7c9aa1","metadata":{"id":"9c7c9aa1"},"source":["## method1: split_mnist"]},{"cell_type":"code","execution_count":null,"id":"d02c2ab4","metadata":{"id":"d02c2ab4","outputId":"7ac967cf-0b7a-4434-9a9e-7b1a46553291"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on device cuda\n"]}],"source":["\n","\n","MNIST_FLATTENED_DIM = 28 * 28\n","LR = 0.001\n","INITIAL_POSTERIOR_VAR = 1e-3\n","\n","device = torch.device(\"cuda\")\n","print(\"Running on device\", device)\n","\n","\n","def split_mnist_original():\n","    \"\"\"\n","    Runs the 'Split MNIST' experiment from the VCL paper, in which each task is\n","    a binary classification task carried out on a subset of the MNIST dataset.\n","    \"\"\"\n","\n","    N_CLASSES = 2\n","    LAYER_WIDTH = 256\n","    N_HIDDEN_LAYERS = 2\n","    N_TASKS = 5\n","    MULTIHEADED = True\n","    CORESET_SIZE = 200\n","    EPOCHS = 10\n","    BATCH_SIZE = 50000\n","    TRAIN_FULL_CORESET = True\n","\n","    transform = Compose([Flatten(), Scale()])\n","\n","    all_accuracies = [[] for _ in range(N_TASKS)]\n","    mean_accuracies = []\n","\n","    # download dataset\n","    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n","    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n","\n","    model = DiscriminativeVCL(\n","        in_size=MNIST_FLATTENED_DIM, out_size=N_CLASSES,\n","        layer_width=LAYER_WIDTH, n_hidden_layers=N_HIDDEN_LAYERS,\n","        n_heads=(N_TASKS if MULTIHEADED else 1),\n","        initial_posterior_var=INITIAL_POSTERIOR_VAR\n","    ).to(device)\n","\n","    coreset = RandomCoreset(size=CORESET_SIZE)\n","\n","\n","    label_to_task_mapping = {\n","        0: 0, 1: 0,\n","        2: 1, 3: 1,\n","        4: 2, 5: 2,\n","        6: 3, 7: 3,\n","        8: 4, 9: 4,\n","    }\n","\n","    if isinstance(mnist_train[0][1], int):\n","        train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n","        test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n","    elif isinstance(mnist_train[0][1], torch.Tensor):\n","        train_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_train])\n","        test_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_test])\n","\n","    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n","    writer = SummaryWriter(summary_logdir)\n","\n","    # each task is a binary classification task for a different pair of digits\n","    binarize_y = lambda y, task: (y == (2 * task + 1)).long()\n","\n","    run_point_estimate_initialisation(model=model, data=mnist_train,\n","                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                                      device=device, multiheaded=MULTIHEADED,\n","                                      lr=LR, task_ids=train_task_ids,\n","                                      y_transform=binarize_y)\n","\n","    for task_idx in range(N_TASKS):\n","\n","        current_task_accuracies, current_mean = run_task(\n","            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n","            test_data=mnist_test, test_task_ids=test_task_ids, coreset=coreset,\n","            task_idx=task_idx, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR,\n","            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n","            y_transform=binarize_y, train_full_coreset=TRAIN_FULL_CORESET,\n","            summary_writer=writer\n","        )\n","        mean_accuracies.append(current_mean)\n","        for i, acc in enumerate(current_task_accuracies):\n","            all_accuracies[i].append(acc)\n","    all_accuracies.append(mean_accuracies)\n","    print(\"All Task Accuracies:\", all_accuracies)\n","    writer.close()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"74969fe2","metadata":{"id":"74969fe2","outputId":"1f04d311-4b51-428a-886d-970d4b70be27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running disc_s_mnist\n","Number of heads: 5\n","Obtaining point estimate for posterior initialisation\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["TASK  0\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 17.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 0 perfomance on task 0 is 99.90543735224587\n","Mean accuracy: 99.90543735224587\n","TASK  1\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 1 perfomance on task 0 is 99.90543735224587\n","After task 1 perfomance on task 1 is 95.5435847208619\n","Mean accuracy: 97.76280971854703\n","TASK  2\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 2 perfomance on task 0 is 99.90543735224587\n","After task 2 perfomance on task 1 is 95.34769833496571\n","After task 2 perfomance on task 2 is 97.2785485592316\n","Mean accuracy: 97.54601226993866\n","TASK  3\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 3 perfomance on task 0 is 99.90543735224587\n","After task 3 perfomance on task 1 is 95.64152791381\n","After task 3 perfomance on task 2 is 97.2785485592316\n","After task 3 perfomance on task 3 is 99.49647532729104\n","Mean accuracy: 98.10402893850568\n","TASK  4\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 4 perfomance on task 0 is 99.90543735224587\n","After task 4 perfomance on task 1 is 95.5435847208619\n","After task 4 perfomance on task 2 is 97.86552828175027\n","After task 4 perfomance on task 3 is 99.29506545820745\n","After task 4 perfomance on task 4 is 95.86485123550176\n","Mean accuracy: 97.71\n","All Task Accuracies: [[99.90543735224587, 99.90543735224587, 99.90543735224587, 99.90543735224587, 99.90543735224587], [95.5435847208619, 95.34769833496571, 95.64152791381, 95.5435847208619], [97.2785485592316, 97.2785485592316, 97.86552828175027], [99.49647532729104, 99.29506545820745], [95.86485123550176], [99.90543735224587, 97.76280971854703, 97.54601226993866, 98.10402893850568, 97.71]]\n"]}],"source":["# random sample corset\n","EXP_OPTIONS = {\n","    'disc_s_mnist': split_mnist_original,\n","    'disc_s_mnist2': split_mnist_corset2,\n","    'disc_s_mnist_order_similar': split_mnist_order_similar,\n","    'disc_s_mnist_order_dissimilar': split_mnist_order_dissimilar\n","}\n","\n","# Set the experiment you want to run here\n","experiment = 'disc_s_mnist'  # Options: 'disc_p_mnist', 'disc_s_mnist', 'disc_s_n_mnist', or 'all' to run all experiments\n","\n","# Run the selected experiment(s)\n","if experiment in EXP_OPTIONS:\n","    print(f\"Running {experiment}\")\n","    EXP_OPTIONS[experiment]()\n","else:\n","    print(f\"Experiment '{experiment}' not found. Available options are: {list(EXP_OPTIONS.keys())}\")\n"]},{"cell_type":"markdown","id":"2463c0b7","metadata":{"id":"2463c0b7"},"source":["## method2: random sample corset"]},{"cell_type":"code","execution_count":null,"id":"nWUdQWM6rAU1","metadata":{"id":"nWUdQWM6rAU1"},"outputs":[],"source":["def split_mnist_corset2():\n","    \"\"\"\n","    Runs the 'Split MNIST' experiment from the VCL paper, in which each task is\n","    a binary classification task carried out on a subset of the MNIST dataset.\n","    \"\"\"\n","\n","    N_CLASSES = 2\n","    LAYER_WIDTH = 256\n","    N_HIDDEN_LAYERS = 2\n","    N_TASKS = 5\n","    MULTIHEADED = True\n","    CORESET_SIZE = 200\n","    EPOCHS = 120\n","    BATCH_SIZE = 50000\n","    TRAIN_FULL_CORESET = True\n","\n","    transform = Compose([Flatten(), Scale()])\n","\n","    all_accuracies = [[] for _ in range(N_TASKS)]\n","    mean_accuracies = []\n","\n","    # download dataset\n","    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n","    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n","\n","    model = DiscriminativeVCL(\n","        in_size=MNIST_FLATTENED_DIM, out_size=N_CLASSES,\n","        layer_width=LAYER_WIDTH, n_hidden_layers=N_HIDDEN_LAYERS,\n","        n_heads=(N_TASKS if MULTIHEADED else 1),\n","        initial_posterior_var=INITIAL_POSTERIOR_VAR\n","    ).to(device)\n","\n","    coreset = RandomCoreset_resample(size=CORESET_SIZE, dataset = mnist_train)\n","\n","\n","    label_to_task_mapping = {\n","        0: 0, 1: 0,\n","        2: 1, 3: 1,\n","        4: 2, 5: 2,\n","        6: 3, 7: 3,\n","        8: 4, 9: 4,\n","    }\n","\n","    if isinstance(mnist_train[0][1], int):\n","        train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n","        test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n","    elif isinstance(mnist_train[0][1], torch.Tensor):\n","        train_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_train])\n","        test_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_test])\n","\n","    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist_2\", datetime.now().strftime('%b%d_%H-%M-%S'))\n","    writer = SummaryWriter(summary_logdir)\n","\n","    # each task is a binary classification task for a different pair of digits\n","    binarize_y = lambda y, task: (y == (2 * task + 1)).long()\n","\n","    run_point_estimate_initialisation(model=model, data=mnist_train,\n","                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                                      device=device, multiheaded=MULTIHEADED,\n","                                      lr=LR, task_ids=train_task_ids,\n","                                      y_transform=binarize_y)\n","\n","    for task_idx in range(N_TASKS):\n","\n","        current_task_accuracies, current_mean =run_task(\n","            model=model,\n","            train_data=mnist_train,\n","            train_task_ids=train_task_ids,\n","            test_data=mnist_test,\n","            test_task_ids=test_task_ids,\n","            coreset=coreset,\n","            task_idx=task_idx,\n","            epochs=EPOCHS,\n","            batch_size=BATCH_SIZE, lr=LR,\n","            save_as=\"disc_s_mnist_2\", device=device, multiheaded=MULTIHEADED,\n","            y_transform=binarize_y, train_full_coreset=TRAIN_FULL_CORESET,\n","            summary_writer=writer\n","        )\n","        mean_accuracies.append(current_mean)\n","        for i, acc in enumerate(current_task_accuracies):\n","            all_accuracies[i].append(acc)\n","    all_accuracies.append(mean_accuracies)\n","    print(\"All Task Accuracies:\", all_accuracies)\n","    writer.close()"]},{"cell_type":"code","execution_count":null,"id":"2b9a0383","metadata":{"id":"2b9a0383","outputId":"5bdb7a2a-24df-483e-d2ec-8ee607395f81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running disc_s_mnist2\n","Number of heads: 5\n","Obtaining point estimate for posterior initialisation\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:59<00:00,  2.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["TASK  0\n","Current classes are: [0, 1]\n","40\n","len(self.coreset) 40\n","non_coreset 12625\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:21<00:00,  5.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:06<00:00, 17.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 0 perfomance on task 0 is 99.90543735224587\n","Mean accuracy: 99.90543735224587\n","TASK  1\n","Current classes are: [0, 1, 2, 3]\n","80\n","len(self.coreset) 80\n","non_coreset 24674\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:41<00:00,  2.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:07<00:00, 17.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 1 perfomance on task 0 is 99.95271867612293\n","After task 1 perfomance on task 1 is 96.91478942213516\n","Mean accuracy: 98.4604281934087\n","TASK  2\n","Current classes are: [0, 1, 2, 3, 4, 5]\n","120\n","len(self.coreset) 120\n","non_coreset 35897\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:58<00:00,  2.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:07<00:00, 16.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 2 perfomance on task 0 is 99.95271867612293\n","After task 2 perfomance on task 1 is 96.32713026444662\n","After task 2 perfomance on task 2 is 93.64994663820704\n","Mean accuracy: 96.76670535566241\n","TASK  3\n","Current classes are: [0, 1, 2, 3, 4, 5, 6, 7]\n","160\n","len(self.coreset) 160\n","non_coreset 48040\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:17<00:00,  1.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:07<00:00, 17.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 3 perfomance on task 0 is 99.95271867612293\n","After task 3 perfomance on task 1 is 96.13124387855044\n","After task 3 perfomance on task 2 is 94.61045891141943\n","After task 3 perfomance on task 3 is 93.95770392749245\n","Mean accuracy: 96.24547835848821\n","TASK  4\n","Current classes are: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","200\n","len(self.coreset) 200\n","non_coreset 59800\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:39<00:00,  1.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:07<00:00, 16.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 4 perfomance on task 0 is 99.90543735224587\n","After task 4 perfomance on task 1 is 94.22135161606268\n","After task 4 perfomance on task 2 is 96.05122732123799\n","After task 4 perfomance on task 3 is 95.36757301107754\n","After task 4 perfomance on task 4 is 87.49369641956632\n","Mean accuracy: 94.66\n","All Task Accuracies: [[99.90543735224587, 99.95271867612293, 99.95271867612293, 99.95271867612293, 99.90543735224587], [96.91478942213516, 96.32713026444662, 96.13124387855044, 94.22135161606268], [93.64994663820704, 94.61045891141943, 96.05122732123799], [93.95770392749245, 95.36757301107754], [87.49369641956632], [99.90543735224587, 98.4604281934087, 96.76670535566241, 96.24547835848821, 94.66]]\n"]}],"source":["EXP_OPTIONS = {\n","    'disc_s_mnist': split_mnist,\n","    'disc_s_mnist2': split_mnist_corset2,\n","    'disc_s_mnist_order_similar': disc_s_mnist_order_similar,\n","    'disc_s_mnist_order_dissimilar': split_mnist_order_dissimilar\n","}\n","\n","# Set the experiment you want to run here\n","experiment = 'disc_s_mnist2'  # Options: 'disc_p_mnist', 'disc_s_mnist', 'disc_s_n_mnist', or 'all' to run all experiments\n","\n","# Run the selected experiment(s)\n","if experiment in EXP_OPTIONS:\n","    print(f\"Running {experiment}\")\n","    EXP_OPTIONS[experiment]()\n","else:\n","    print(f\"Experiment '{experiment}' not found. Available options are: {list(EXP_OPTIONS.keys())}\")\n"]},{"cell_type":"code","execution_count":null,"id":"4d4c340d","metadata":{"id":"4d4c340d"},"outputs":[],"source":["# reorder\n","def binarize_y(y, task, task_to_labels):\n","    class_0, class_1 = task_to_labels[task]\n","    return (y == class_1).long()\n","\n","# reorder\n","def split_mnist_order_similar():\n","\n","    N_CLASSES = 2 # TODO does it make sense to do binary classification with out_size=2 ?\n","    LAYER_WIDTH = 256\n","    N_HIDDEN_LAYERS = 2\n","    N_TASKS = 5\n","    MULTIHEADED = True\n","    CORESET_SIZE = 200\n","    EPOCHS = 120\n","#     BATCH_SIZE = 50000\n","    BATCH_SIZE = 50000\n","\n","    TRAIN_FULL_CORESET = True\n","\n","    transform = Compose([Flatten(), Scale()])\n","\n","    all_accuracies = [[] for _ in range(N_TASKS)]\n","    mean_accuracies = []\n","\n","    # download dataset\n","    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n","    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n","\n","    model = DiscriminativeVCL(\n","        in_size=MNIST_FLATTENED_DIM, out_size=N_CLASSES,\n","        layer_width=LAYER_WIDTH, n_hidden_layers=N_HIDDEN_LAYERS,\n","        n_heads=(N_TASKS if MULTIHEADED else 1),\n","        initial_posterior_var=INITIAL_POSTERIOR_VAR\n","    ).to(device)\n","\n","    coreset = RandomCoreset(size=CORESET_SIZE)\n","# similar\n","    label_to_task_mapping = {\n","        2: 0, 3: 0,\n","        5: 1, 6: 1,\n","        8: 2, 9: 2,\n","        0: 3, 4: 3,\n","        7: 4, 1: 4,\n","    }\n","\n","    task_to_labels = {task: [] for task in range(N_TASKS)}\n","    for label, task in label_to_task_mapping.items():\n","        task_to_labels[task].append(label)\n","    for task, labels in task_to_labels.items():\n","        task_to_labels[task] = sorted(labels)\n","\n","    train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n","    test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n","\n","    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n","    writer = SummaryWriter(summary_logdir)\n","\n","    # each task is a binary classification task for a different pair of digits\n","#     binarize_y = lambda y, task: (y == (2 * task + 1)).long()\n","    bin_y = lambda y, task: binarize_y(y, task, task_to_labels)\n","\n","    run_point_estimate_initialisation(model=model, data=mnist_train,\n","                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                                      device=device, multiheaded=MULTIHEADED,\n","                                      lr=LR, task_ids=train_task_ids,\n","                                      y_transform=bin_y)\n","\n","    for task_idx in range(N_TASKS):\n","        current_task_accuracies, current_mean = run_task(\n","            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n","            test_data=mnist_test, test_task_ids=test_task_ids, coreset=coreset,\n","            task_idx=task_idx, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR,\n","            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n","            y_transform=bin_y, train_full_coreset=TRAIN_FULL_CORESET,\n","            summary_writer=writer\n","        )\n","        mean_accuracies.append(current_mean)\n","        for i, acc in enumerate(current_task_accuracies):\n","            all_accuracies[i].append(acc)\n","    all_accuracies.append(mean_accuracies)\n","    print(\"All Task Accuracies:\", all_accuracies)\n","\n","    writer.close()"]},{"cell_type":"markdown","id":"8d51986b","metadata":{"id":"8d51986b"},"source":["## method3: similar within task"]},{"cell_type":"code","execution_count":null,"id":"a16e3f69","metadata":{"id":"a16e3f69","outputId":"b20cad22-20f5-4dc7-9dc0-52afaaed84e6","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running disc_s_mnist_order\n","Number of heads: 5\n","Obtaining point estimate for posterior initialisation\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:54<00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["TASK  0\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:04<00:00,  1.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:06<00:00, 17.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 0 perfomance on task 0 is 99.75514201762978\n","Mean accuracy: 99.75514201762978\n","TASK  1\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:00<00:00,  1.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:13<00:00,  8.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 1 perfomance on task 0 is 99.51028403525955\n","After task 1 perfomance on task 1 is 98.21621621621621\n","Mean accuracy: 98.89516957862281\n","TASK  2\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:03<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:21<00:00,  5.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 2 perfomance on task 0 is 98.87365328109696\n","After task 2 perfomance on task 1 is 98.27027027027027\n","After task 2 perfomance on task 2 is 97.68028240040343\n","Mean accuracy: 98.28085106382979\n","TASK  3\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:03<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:29<00:00,  4.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 3 perfomance on task 0 is 99.31439764936337\n","After task 3 perfomance on task 1 is 98.16216216216216\n","After task 3 perfomance on task 2 is 98.23499747856782\n","After task 3 perfomance on task 3 is 99.84709480122324\n","Mean accuracy: 98.90264131683043\n","TASK  4\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:10<00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:37<00:00,  3.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 4 perfomance on task 0 is 99.41234084231147\n","After task 4 perfomance on task 1 is 98.16216216216216\n","After task 4 perfomance on task 2 is 95.96570852244075\n","After task 4 perfomance on task 3 is 99.84709480122324\n","After task 4 perfomance on task 4 is 99.4914470642626\n","Mean accuracy: 98.6\n","All Task Accuracies: [[99.75514201762978, 99.51028403525955, 98.87365328109696, 99.31439764936337, 99.41234084231147], [98.21621621621621, 98.27027027027027, 98.16216216216216, 98.16216216216216], [97.68028240040343, 98.23499747856782, 95.96570852244075], [99.84709480122324, 99.84709480122324], [99.4914470642626], [99.75514201762978, 98.89516957862281, 98.28085106382979, 98.90264131683043, 98.6]]\n"]}],"source":["\n","\n","EXP_OPTIONS = {\n","    'disc_s_mnist': split_mnist_original,\n","    'disc_s_mnist2': split_mnist_corset2,\n","    'disc_s_mnist_order_similar': split_mnist_order_similar,\n","    'disc_s_mnist_order_dissimilar': split_mnist_order_dissimilar\n","}\n","\n","# Set the experiment you want to run here\n","experiment = 'disc_s_mnist_order_similar'  # Options: 'disc_p_mnist', 'disc_s_mnist', 'disc_s_n_mnist', or 'all' to run all experiments\n","\n","# Run the selected experiment(s)\n","if experiment in EXP_OPTIONS:\n","    print(f\"Running {experiment}\")\n","    EXP_OPTIONS[experiment]()\n","else:\n","    print(f\"Experiment '{experiment}' not found. Available options are: {list(EXP_OPTIONS.keys())}\")\n"]},{"cell_type":"markdown","id":"c606ebf3","metadata":{"id":"c606ebf3"},"source":["## method4: dissimilar within task"]},{"cell_type":"code","execution_count":null,"id":"vpqdoIyzp1-P","metadata":{"id":"vpqdoIyzp1-P"},"outputs":[],"source":["# reorder\n","def split_mnist_order_dissimilar():\n","\n","    N_CLASSES = 2 # TODO does it make sense to do binary classification with out_size=2 ?\n","    LAYER_WIDTH = 256\n","    N_HIDDEN_LAYERS = 2\n","    N_TASKS = 5\n","    MULTIHEADED = True\n","    CORESET_SIZE = 200\n","    EPOCHS = 120\n","#     BATCH_SIZE = 50000\n","    BATCH_SIZE = 50000\n","\n","    TRAIN_FULL_CORESET = True\n","\n","    transform = Compose([Flatten(), Scale()])\n","\n","    all_accuracies = [[] for _ in range(N_TASKS)]\n","    mean_accuracies = []\n","\n","    # download dataset\n","    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n","    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n","\n","    model = DiscriminativeVCL(\n","        in_size=MNIST_FLATTENED_DIM, out_size=N_CLASSES,\n","        layer_width=LAYER_WIDTH, n_hidden_layers=N_HIDDEN_LAYERS,\n","        n_heads=(N_TASKS if MULTIHEADED else 1),\n","        initial_posterior_var=INITIAL_POSTERIOR_VAR\n","    ).to(device)\n","\n","    coreset = RandomCoreset(size=CORESET_SIZE)\n","# # dissimilar\n","    label_to_task_mapping = {\n","        2: 0, 8: 0,\n","        5: 1, 9: 1,\n","        6: 2, 7: 2,\n","        0: 3, 4: 3,\n","        3: 4, 1: 4,\n","    }\n","\n","    task_to_labels = {task: [] for task in range(N_TASKS)}\n","    for label, task in label_to_task_mapping.items():\n","        task_to_labels[task].append(label)\n","    for task, labels in task_to_labels.items():\n","        task_to_labels[task] = sorted(labels)\n","\n","    train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n","    test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n","\n","    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n","    writer = SummaryWriter(summary_logdir)\n","\n","\n","    bin_y = lambda y, task: binarize_y(y, task, task_to_labels)\n","\n","    run_point_estimate_initialisation(model=model, data=mnist_train,\n","                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                                      device=device, multiheaded=MULTIHEADED,\n","                                      lr=LR, task_ids=train_task_ids,\n","                                      y_transform=bin_y)\n","\n","    for task_idx in range(N_TASKS):\n","        current_task_accuracies, current_mean = run_task(\n","            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n","            test_data=mnist_test, test_task_ids=test_task_ids, coreset=coreset,\n","            task_idx=task_idx, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR,\n","            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n","            y_transform=bin_y, train_full_coreset=TRAIN_FULL_CORESET,\n","            summary_writer=writer\n","        )\n","        mean_accuracies.append(current_mean)\n","        for i, acc in enumerate(current_task_accuracies):\n","            all_accuracies[i].append(acc)\n","    all_accuracies.append(mean_accuracies)\n","    print(\"All Task Accuracies:\", all_accuracies)\n","\n","    writer.close()"]},{"cell_type":"code","execution_count":null,"id":"3e46bc85","metadata":{"id":"3e46bc85","outputId":"b12eb6d3-92ff-42b1-82dc-2b9769015b48"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running disc_s_mnist_order\n","Number of heads: 5\n","Obtaining point estimate for posterior initialisation\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:53<00:00,  2.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["TASK  0\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:03<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:06<00:00, 19.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 0 perfomance on task 0 is 99.2023928215354\n","Mean accuracy: 99.2023928215354\n","TASK  1\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:01<00:00,  1.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:13<00:00,  8.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 1 perfomance on task 0 is 99.10269192422732\n","After task 1 perfomance on task 1 is 98.73750657548659\n","Mean accuracy: 98.92500639877143\n","TASK  2\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:05<00:00,  1.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:21<00:00,  5.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 2 perfomance on task 0 is 99.05284147557327\n","After task 2 perfomance on task 1 is 98.79011046817465\n","After task 2 perfomance on task 2 is 99.49647532729104\n","Mean accuracy: 99.11759714916002\n","TASK  3\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:03<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:28<00:00,  4.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 3 perfomance on task 0 is 99.15254237288136\n","After task 3 perfomance on task 1 is 98.52709100473434\n","After task 3 perfomance on task 2 is 99.29506545820745\n","After task 3 perfomance on task 3 is 99.64322120285424\n","Mean accuracy: 99.15977084659453\n","TASK  4\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:10<00:00,  1.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:34<00:00,  3.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 4 perfomance on task 0 is 98.70388833499501\n","After task 4 perfomance on task 1 is 98.47448711204629\n","After task 4 perfomance on task 2 is 99.29506545820745\n","After task 4 perfomance on task 3 is 99.59225280326197\n","After task 4 perfomance on task 4 is 99.44055944055944\n","Mean accuracy: 99.11\n","All Task Accuracies: [[99.2023928215354, 99.10269192422732, 99.05284147557327, 99.15254237288136, 98.70388833499501], [98.73750657548659, 98.79011046817465, 98.52709100473434, 98.47448711204629], [99.49647532729104, 99.29506545820745, 99.29506545820745], [99.64322120285424, 99.59225280326197], [99.44055944055944], [99.2023928215354, 98.92500639877143, 99.11759714916002, 99.15977084659453, 99.11]]\n"]}],"source":["EXP_OPTIONS = {\n","    'disc_s_mnist': split_mnist_original,\n","    'disc_s_mnist2': split_mnist_corset2,\n","    'disc_s_mnist_order_similar': split_mnist_order_similar,\n","    'disc_s_mnist_order_dissimilar': split_mnist_order_dissimilar\n","}\n","\n","# Set the experiment you want to run here\n","experiment = 'disc_s_mnist_order_dissimilar'  # Options: 'disc_p_mnist', 'disc_s_mnist', 'disc_s_n_mnist', or 'all' to run all experiments\n","\n","# Run the selected experiment(s)\n","\n","if experiment in EXP_OPTIONS:\n","    print(f\"Running {experiment}\")\n","    EXP_OPTIONS[experiment]()\n","else:\n","    print(f\"Experiment '{experiment}' not found. Available options are: {list(EXP_OPTIONS.keys())}\")\n"]},{"cell_type":"code","execution_count":null,"id":"486bc7a2","metadata":{"id":"486bc7a2"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e6520f7a","metadata":{"id":"e6520f7a"},"source":["## method5: scaling factor"]},{"cell_type":"code","execution_count":null,"id":"b47a5698","metadata":{"id":"b47a5698","outputId":"22cdd72e-5f40-46f1-d960-60b2f0f3a7ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on device cuda\n"]}],"source":["\n","MNIST_FLATTENED_DIM = 28 * 28\n","LR = 0.001\n","INITIAL_POSTERIOR_VAR = 1e-3\n","\n","device = torch.device(\"cuda\")\n","print(\"Running on device\", device)\n","\n","def split_mnist_new():\n","    \"\"\"\n","    Runs the 'Split MNIST' experiment from the VCL paper, in which each task is\n","    a binary classification task carried out on a subset of the MNIST dataset.\n","    \"\"\"\n","\n","    N_CLASSES = 2\n","    LAYER_WIDTH = 256\n","    N_HIDDEN_LAYERS = 2\n","    N_TASKS = 5\n","    MULTIHEADED = True\n","    CORESET_SIZE = 200\n","    EPOCHS = 120\n","    BATCH_SIZE = 50000\n","    TRAIN_FULL_CORESET = True\n","\n","    transform = Compose([Flatten(), Scale()])\n","\n","    all_accuracies = [[] for _ in range(N_TASKS)]\n","    mean_accuracies = []\n","    prev_mean_accuracy = None\n","\n","    # download dataset\n","    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n","    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n","\n","    model = DiscriminativeVCL(\n","        in_size=MNIST_FLATTENED_DIM, out_size=N_CLASSES,\n","        layer_width=LAYER_WIDTH, n_hidden_layers=N_HIDDEN_LAYERS,\n","        n_heads=(N_TASKS if MULTIHEADED else 1),\n","        initial_posterior_var=INITIAL_POSTERIOR_VAR\n","    ).to(device)\n","\n","    coreset = RandomCoreset(size=CORESET_SIZE)\n","\n","\n","    label_to_task_mapping = {\n","        0: 0, 1: 0,\n","        2: 1, 3: 1,\n","        4: 2, 5: 2,\n","        6: 3, 7: 3,\n","        8: 4, 9: 4,\n","    }\n","\n","    if isinstance(mnist_train[0][1], int):\n","        train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n","        test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n","    elif isinstance(mnist_train[0][1], torch.Tensor):\n","        train_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_train])\n","        test_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_test])\n","\n","    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n","    writer = SummaryWriter(summary_logdir)\n","\n","    # each task is a binary classification task for a different pair of digits\n","    binarize_y = lambda y, task: (y == (2 * task + 1)).long()\n","\n","    run_point_estimate_initialisation(model=model, data=mnist_train,\n","                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                                      device=device, multiheaded=MULTIHEADED,\n","                                      lr=LR, task_ids=train_task_ids,\n","                                      y_transform=binarize_y)\n","\n","    for task_idx in range(N_TASKS):\n","\n","        current_task_accuracies, current_mean = run_task_scale(\n","            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n","            test_data=mnist_test, test_task_ids=test_task_ids, coreset=coreset,\n","            task_idx=task_idx, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR,\n","            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n","            y_transform=binarize_y, train_full_coreset=TRAIN_FULL_CORESET,\n","            summary_writer=writer\n","        )\n","        current_last_accuracy = current_task_accuracies[-1]\n","        model.adjust_scaling_factors(current_mean, current_last_accuracy)\n","\n","        mean_accuracies.append(current_mean)\n","        for i, acc in enumerate(current_task_accuracies):\n","            all_accuracies[i].append(acc)\n","    all_accuracies.append(mean_accuracies)\n","    print(\"All Task Accuracies:\", all_accuracies)\n","    writer.close()"]},{"cell_type":"code","execution_count":null,"id":"8a3f3e62","metadata":{"scrolled":true,"id":"8a3f3e62","outputId":"3b9b4284-62d5-4a10-93e3-67b6658d5a05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running disc_s_mnist_order_scale\n","Number of heads: 5\n","Obtaining point estimate for posterior initialisation\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:58<00:00,  2.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["TASK  0\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:09<00:00,  1.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:06<00:00, 17.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 0 perfomance on task 0 is 99.95271867612293\n","Mean accuracy: 99.95271867612293\n","alpha 1.0\n","beta 1.0\n","TASK  1\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:05<00:00,  1.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:14<00:00,  8.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 1 perfomance on task 0 is 99.95271867612293\n","After task 1 perfomance on task 1 is 97.94319294809011\n","Mean accuracy: 98.96560019244647\n","alpha 1.1022407244356358\n","beta 0.8977592755643642\n","TASK  2\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:01<00:00,  1.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:21<00:00,  5.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 2 perfomance on task 0 is 99.95271867612293\n","After task 2 perfomance on task 1 is 97.35553379040157\n","After task 2 perfomance on task 2 is 99.51974386339381\n","Mean accuracy: 98.93881611673022\n","alpha 0.9419072253336409\n","beta 1.058092774666359\n","TASK  3\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:06<00:00,  1.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:28<00:00,  4.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 3 perfomance on task 0 is 99.90543735224587\n","After task 3 perfomance on task 1 is 97.25759059745347\n","After task 3 perfomance on task 2 is 99.41302027748132\n","After task 3 perfomance on task 3 is 99.69788519637463\n","Mean accuracy: 99.06448796307846\n","alpha 0.9366602766703835\n","beta 1.0633397233296165\n","TASK  4\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:04<00:00,  1.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:34<00:00,  3.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 4 perfomance on task 0 is 99.90543735224587\n","After task 4 perfomance on task 1 is 96.42507345739472\n","After task 4 perfomance on task 2 is 99.09284951974387\n","After task 4 perfomance on task 3 is 98.99295065458207\n","After task 4 perfomance on task 4 is 97.52899646999495\n","Mean accuracy: 98.39\n","alpha 1.086100353000505\n","beta 0.913899646999495\n","All Task Accuracies: [[99.95271867612293, 99.95271867612293, 99.95271867612293, 99.90543735224587, 99.90543735224587], [97.94319294809011, 97.35553379040157, 97.25759059745347, 96.42507345739472], [99.51974386339381, 99.41302027748132, 99.09284951974387], [99.69788519637463, 98.99295065458207], [97.52899646999495], [99.95271867612293, 98.96560019244647, 98.93881611673022, 99.06448796307846, 98.39]]\n"]}],"source":["# random sample corset\n","EXP_OPTIONS = {\n","    'disc_s_mnist': split_mnist_original,\n","    'disc_s_mnist2': split_mnist_corset2,\n","    'disc_s_mnist_order_similar': split_mnist_order_similar,\n","    'disc_s_mnist_order_dissimilar': split_mnist_order_dissimilar,\n","    'disc_s_mnist_order_scale': split_mnist_new\n","}\n","\n","# Set the experiment you want to run here\n","experiment = 'disc_s_mnist_order_scale'  # Options: 'disc_p_mnist', 'disc_s_mnist', 'disc_s_n_mnist', or 'all' to run all experiments\n","\n","# Run the selected experiment(s)\n","if experiment == 'all':\n","    for exp_name, exp_func in EXP_OPTIONS.items():\n","        print(f\"Running {exp_name}\")\n","        exp_func()\n","else:\n","    if experiment in EXP_OPTIONS:\n","        print(f\"Running {experiment}\")\n","        EXP_OPTIONS[experiment]()\n","    else:\n","        print(f\"Experiment '{experiment}' not found. Available options are: {list(EXP_OPTIONS.keys())}\")\n"]},{"cell_type":"markdown","id":"d7e6c381","metadata":{"id":"d7e6c381"},"source":["## method6: scaling factor+order"]},{"cell_type":"code","execution_count":null,"id":"b7744867","metadata":{"id":"b7744867","outputId":"bd260b09-3727-4158-9a1c-4a4287615a20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on device cuda\n"]}],"source":["\n","MNIST_FLATTENED_DIM = 28 * 28\n","LR = 0.001\n","INITIAL_POSTERIOR_VAR = 1e-3\n","\n","device = torch.device(\"cuda\")\n","print(\"Running on device\", device)\n","\n","def binarize_y(y, task, task_to_labels):\n","    class_0, class_1 = task_to_labels[task]\n","    return (y == class_1).long()\n","\n","# reorder\n","\n","def split_mnist_factor_dissimilar():\n","    \"\"\"\n","    Runs the 'Split MNIST' experiment from the VCL paper, in which each task is\n","    a binary classification task carried out on a subset of the MNIST dataset.\n","    \"\"\"\n","\n","    N_CLASSES = 2\n","    LAYER_WIDTH = 256\n","    N_HIDDEN_LAYERS = 2\n","    N_TASKS = 5\n","    MULTIHEADED = True\n","    CORESET_SIZE = 200\n","    EPOCHS = 120\n","    BATCH_SIZE = 50000\n","    TRAIN_FULL_CORESET = True\n","\n","    transform = Compose([Flatten(), Scale()])\n","\n","    all_accuracies = [[] for _ in range(N_TASKS)]\n","    mean_accuracies = []\n","    prev_mean_accuracy = None\n","\n","    # download dataset\n","    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n","    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n","\n","    model = DiscriminativeVCL(\n","        in_size=MNIST_FLATTENED_DIM, out_size=N_CLASSES,\n","        layer_width=LAYER_WIDTH, n_hidden_layers=N_HIDDEN_LAYERS,\n","        n_heads=(N_TASKS if MULTIHEADED else 1),\n","        initial_posterior_var=INITIAL_POSTERIOR_VAR\n","    ).to(device)\n","\n","    coreset = RandomCoreset(size=CORESET_SIZE)\n","\n","\n","    label_to_task_mapping = {\n","        2: 0, 8: 0,\n","        5: 1, 9: 1,\n","        6: 2, 7: 2,\n","        0: 3, 4: 3,\n","        3: 4, 1: 4,\n","    }\n","\n","    task_to_labels = {task: [] for task in range(N_TASKS)}\n","    for label, task in label_to_task_mapping.items():\n","        task_to_labels[task].append(label)\n","    for task, labels in task_to_labels.items():\n","        task_to_labels[task] = sorted(labels)\n","\n","    train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n","    test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n","\n","    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n","    writer = SummaryWriter(summary_logdir)\n","\n","    # each task is a binary classification task for a different pair of digits\n","    bin_y = lambda y, task: binarize_y(y, task, task_to_labels)\n","\n","    run_point_estimate_initialisation(model=model, data=mnist_train,\n","                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                                      device=device, multiheaded=MULTIHEADED,\n","                                      lr=LR, task_ids=train_task_ids,\n","                                      y_transform=bin_y)\n","\n","    for task_idx in range(N_TASKS):\n","\n","        current_task_accuracies, current_mean = run_task_scale(\n","            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n","            test_data=mnist_test, test_task_ids=test_task_ids, coreset=coreset,\n","            task_idx=task_idx, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR,\n","            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n","            y_transform=bin_y, train_full_coreset=TRAIN_FULL_CORESET,\n","            summary_writer=writer\n","        )\n","        current_last_accuracy = current_task_accuracies[-1]\n","        model.adjust_scaling_factors(current_mean, current_last_accuracy)\n","\n","        mean_accuracies.append(current_mean)\n","        for i, acc in enumerate(current_task_accuracies):\n","            all_accuracies[i].append(acc)\n","    all_accuracies.append(mean_accuracies)\n","    print(\"All Task Accuracies:\", all_accuracies)\n","    writer.close()"]},{"cell_type":"code","execution_count":null,"id":"2967d374","metadata":{"id":"2967d374","outputId":"fab0aead-c985-445a-dad0-0351f5cc89cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running disc_s_mnist_order_scale_dis\n","Number of heads: 5\n","Obtaining point estimate for posterior initialisation\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:56<00:00,  2.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["TASK  0\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:04<00:00,  1.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:06<00:00, 17.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 0 perfomance on task 0 is 99.45164506480559\n","Mean accuracy: 99.45164506480559\n","alpha 1.0\n","beta 1.0\n","TASK  1\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:03<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:13<00:00,  8.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 1 perfomance on task 0 is 99.3519441674975\n","After task 1 perfomance on task 1 is 98.15886375591793\n","Mean accuracy: 98.77143588431021\n","alpha 1.0612572128392272\n","beta 0.9387427871607728\n","TASK  2\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:07<00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:20<00:00,  5.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 2 perfomance on task 0 is 98.85343968095712\n","After task 2 perfomance on task 1 is 98.36927932667017\n","After task 2 perfomance on task 2 is 99.34541792547834\n","Mean accuracy: 98.86305786526387\n","alpha 0.9517639939785525\n","beta 1.0482360060214475\n","TASK  3\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:04<00:00,  1.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:28<00:00,  4.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 3 perfomance on task 0 is 98.90329012961116\n","After task 3 perfomance on task 1 is 98.26407154129406\n","After task 3 perfomance on task 2 is 99.39577039274924\n","After task 3 perfomance on task 3 is 99.59225280326197\n","Mean accuracy: 99.04519414385742\n","alpha 0.9452941340595444\n","beta 1.0547058659404556\n","TASK  4\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [01:11<00:00,  1.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CORESET TRAIN\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:35<00:00,  3.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["After task 4 perfomance on task 0 is 99.40179461615155\n","After task 4 perfomance on task 1 is 98.26407154129406\n","After task 4 perfomance on task 2 is 99.29506545820745\n","After task 4 perfomance on task 3 is 99.64322120285424\n","After task 4 perfomance on task 4 is 99.39393939393939\n","Mean accuracy: 99.21\n","alpha 0.9816060606060603\n","beta 1.0183939393939396\n","All Task Accuracies: [[99.45164506480559, 99.3519441674975, 98.85343968095712, 98.90329012961116, 99.40179461615155], [98.15886375591793, 98.36927932667017, 98.26407154129406, 98.26407154129406], [99.34541792547834, 99.39577039274924, 99.29506545820745], [99.59225280326197, 99.64322120285424], [99.39393939393939], [99.45164506480559, 98.77143588431021, 98.86305786526387, 99.04519414385742, 99.21]]\n"]}],"source":["split_mnist_factor_dissimilar\n","# random sample corset\n","EXP_OPTIONS = {\n","#     'disc_s_mnist': split_mnist_original,\n","#     'disc_s_mnist2': split_mnist_corset2,\n","#     'disc_s_mnist_order_similar': split_mnist_order_similar,\n","#     'disc_s_mnist_order_dissimilar': split_mnist_order_dissimilar,\n","#     'disc_s_mnist_order_scale': split_mnist_new,\n","    'disc_s_mnist_order_scale_dis': split_mnist_factor_dissimilar,\n","}\n","\n","# Set the experiment you want to run here\n","experiment = 'disc_s_mnist_order_scale_dis'  # Options: 'disc_p_mnist', 'disc_s_mnist', 'disc_s_n_mnist', or 'all' to run all experiments\n","\n","# Run the selected experiment(s)\n","if experiment == 'all':\n","    for exp_name, exp_func in EXP_OPTIONS.items():\n","        print(f\"Running {exp_name}\")\n","        exp_func()\n","else:\n","    if experiment in EXP_OPTIONS:\n","        print(f\"Running {experiment}\")\n","        EXP_OPTIONS[experiment]()\n","    else:\n","        print(f\"Experiment '{experiment}' not found. Available options are: {list(EXP_OPTIONS.keys())}\")\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":5}